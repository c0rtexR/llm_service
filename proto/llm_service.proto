syntax = "proto3";

package llm.v1;

option go_package = "llmservice/proto";

// LLMService provides access to various LLM providers
service LLMService {
  // Invoke sends a request to an LLM provider and returns a response
  rpc Invoke(LLMRequest) returns (LLMResponse);
  
  // InvokeStream sends a request to an LLM provider and returns a stream of responses
  rpc InvokeStream(LLMRequest) returns (stream LLMStreamResponse);
}

// LLMRequest represents a request to an LLM provider
message LLMRequest {
  // Provider specifies which LLM provider to use (e.g., "openai", "anthropic")
  string provider = 1;
  
  // Model specifies which model to use (e.g., "gpt-3.5-turbo", "claude-2")
  string model = 2;
  
  // Messages contains the conversation history
  repeated ChatMessage messages = 3;
  
  // Temperature controls randomness in the response (0.0 to 1.0)
  float temperature = 4;
  
  // MaxTokens limits the length of the response
  int32 max_tokens = 5;
  
  // TopP controls diversity via nucleus sampling
  float top_p = 6;
  
  // CacheControl specifies caching behavior
  CacheControl cache_control = 7;

  // TopK controls diversity by limiting to k most likely tokens
  int32 top_k = 8;
}

// ChatMessage represents a single message in the conversation
message ChatMessage {
  // Role specifies who sent the message (e.g., "system", "user", "assistant")
  string role = 1;
  
  // Content contains the actual message text
  string content = 2;
}

// CacheControl specifies caching behavior for the request
message CacheControl {
  // UseCache indicates whether to use cached responses
  bool use_cache = 1;
  
  // TTL specifies how long to cache the response (in seconds)
  int32 ttl = 2;
}

// LLMResponse represents a response from an LLM provider
message LLMResponse {
  // Content contains the response text
  string content = 1;
  
  // Usage provides token usage statistics
  UsageInfo usage = 2;
}

// LLMStreamResponse represents a chunk of a streaming response
message LLMStreamResponse {
  // Type indicates what kind of response this is
  ResponseType type = 1;
  
  // Content contains the response text (for TYPE_CONTENT)
  string content = 2;
  
  // FinishReason indicates why the response ended (for TYPE_FINISH_REASON)
  string finish_reason = 3;
  
  // Usage provides token usage statistics (for TYPE_USAGE)
  UsageInfo usage = 4;
}

// ResponseType indicates what kind of stream response this is
enum ResponseType {
  // TYPE_UNSPECIFIED is the default value
  TYPE_UNSPECIFIED = 0;
  
  // TYPE_CONTENT indicates this response contains content text
  TYPE_CONTENT = 1;
  
  // TYPE_FINISH_REASON indicates this response contains the finish reason
  TYPE_FINISH_REASON = 2;
  
  // TYPE_USAGE indicates this response contains usage statistics
  TYPE_USAGE = 3;
}

// UsageInfo provides token usage statistics
message UsageInfo {
  // PromptTokens is the number of tokens in the prompt
  int32 prompt_tokens = 1;
  
  // CompletionTokens is the number of tokens in the completion
  int32 completion_tokens = 2;
  
  // TotalTokens is the total number of tokens used
  int32 total_tokens = 3;
} 